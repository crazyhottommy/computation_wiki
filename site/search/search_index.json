{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"computation wiki page for CIDC \u00b6","title":"Home"},{"location":"#computation-wiki-page-for-cidc","text":"","title":"computation wiki page for CIDC"},{"location":"RIMA/build-RIMA-reference/","text":"We have prepared the RIMA reference folder with the current version we used. You can directly download from Iris server: \u00b6 # We have nearly 69G reference files wget http://cistrome.org/~lyang/ref.tar.gz Steps to make your own custom reference files for RIMA pipeline \u00b6 Reference fasta \u00b6 The human GDC hg38 fasta file is downloaded from GDC website . Gene annotation file (gtf) \u00b6 The human gtf annotation file is downloaded from GENCODE website . The current annotation file we used is V27. build STAR index \u00b6 conda activate rna ## STAR Version: STAR_2.6.1d STAR --runThreadN 16 --runMode genomeGenerate --genomeDir ./ref_files/v27_index --genomeFastaFiles GRCh38.d1.vd1.CIDC.fa --sjdbGTFfile gencode.v27.annotation.gtf ... 00 :04:54 ..... started STAR run 00 :04:54 ... starting to generate Genome files 00 :05:57 ... starting to sort Suffix Array. This may take a long time... 00 :06:11 ... sorting Suffix Array chunks and saving them to disk... 00 :17:43 ... loading chunks from disk, packing SA... 00 :19:31 ... finished generating suffix array 00 :19:31 ... generating Suffix Array index 00 :23:20 ... completed Suffix Array index 00 :23:20 ..... processing annotations GTF 00 :23:35 ..... inserting junctions into the genome indices 00 :26:49 ... writing Genome to disk ... 00 :27:06 ... writing Suffix Array to disk ... 00 :28:53 ... writing SAindex to disk 00 :29:05 ..... finished successfully RSeQC reference files \u00b6 We download the human annotation bed file including the whole genome bed file, and house keeping bed file from RSeQC page from sourcforge website . ./ref_files/refseqGenes.bed ./ref_files/housekeeping_refseqGenes.bed build salmon index \u00b6 conda activate rna ## salmon Version: salmon 1.1.0 salmon index -t GRCh38.d1.vd1.CIDC.fa -i salmon_index ... index [ \"salmon_index\" ] did not previously exist . . . creating it [ jLog ] [ info ] building index [ jointLog ] [ info ] [ Step 1 of 4 ] : counting k-mers [ jointLog ] [ info ] Replaced 164 ,553,847 non-ATCG nucleotides [ jointLog ] [ info ] Clipped poly-A tails from 0 transcripts [ jointLog ] [ info ] Building rank-select dictionary and saving to disk [ jointLog ] [ info ] done Elapsed time: 0 .191866s [ jointLog ] [ info ] Writing sequence data to file . . . [ jointLog ] [ info ] done Elapsed time: 1 .91244s [ jointLog ] [ info ] Building 64 -bit suffix array ( length of generalized text is 3 ,088,286,426 ) [ jointLog ] [ info ] Building suffix array . . . success saving to disk . . . done Elapsed time: 18 .3072s done Elapsed time: 703 .843s GMT file for gene set analysis \u00b6 The GMT file is downloaded from BROAD release page . The current GMT file we used is \"c2.cp.kegg.v6.1.symbols.gmt\" STAR-Fusion genome resource lib \u00b6 The genome resource lib is downloaded from BROAD release page . The current lib we used is GRCh38_v22_CTAT_lib. You can also prep it for use with STAR-fusion. More details, read: https://github.com/STAR-Fusion/STAR-Fusion/wiki/installing-star-fusion Centrifuge index \u00b6 The human Centrifuge index is downloaded from Centrifuge website . The current index we used is p_compressed+h+v that includes human genome, prokaryotic genomes, and viral genomes. You can also build your own custom Centrifuge index. More details, read: https://github.com/DaehwanKimLab/centrifuge TRUST4 reference files \u00b6 TRUST4 reference files includes 1. TCR, BCR genomic sequence fasta file; 2. Reference database sequence containing annotation information. hg38_bcrtcr.fa human_IMGT+C.fa These reference files can directlt be downloaded from TRUST4 github .","title":"Build RIMA references"},{"location":"RIMA/build-RIMA-reference/#we-have-prepared-the-rima-reference-folder-with-the-current-version-we-used-you-can-directly-download-from-iris-server","text":"# We have nearly 69G reference files wget http://cistrome.org/~lyang/ref.tar.gz","title":"We have prepared the RIMA reference folder with the current version we used. You can directly download from Iris server:"},{"location":"RIMA/build-RIMA-reference/#steps-to-make-your-own-custom-reference-files-for-rima-pipeline","text":"","title":"Steps to make your own custom reference files for RIMA pipeline"},{"location":"RIMA/build-RIMA-reference/#reference-fasta","text":"The human GDC hg38 fasta file is downloaded from GDC website .","title":"Reference fasta"},{"location":"RIMA/build-RIMA-reference/#gene-annotation-file-gtf","text":"The human gtf annotation file is downloaded from GENCODE website . The current annotation file we used is V27.","title":"Gene annotation file (gtf)"},{"location":"RIMA/build-RIMA-reference/#build-star-index","text":"conda activate rna ## STAR Version: STAR_2.6.1d STAR --runThreadN 16 --runMode genomeGenerate --genomeDir ./ref_files/v27_index --genomeFastaFiles GRCh38.d1.vd1.CIDC.fa --sjdbGTFfile gencode.v27.annotation.gtf ... 00 :04:54 ..... started STAR run 00 :04:54 ... starting to generate Genome files 00 :05:57 ... starting to sort Suffix Array. This may take a long time... 00 :06:11 ... sorting Suffix Array chunks and saving them to disk... 00 :17:43 ... loading chunks from disk, packing SA... 00 :19:31 ... finished generating suffix array 00 :19:31 ... generating Suffix Array index 00 :23:20 ... completed Suffix Array index 00 :23:20 ..... processing annotations GTF 00 :23:35 ..... inserting junctions into the genome indices 00 :26:49 ... writing Genome to disk ... 00 :27:06 ... writing Suffix Array to disk ... 00 :28:53 ... writing SAindex to disk 00 :29:05 ..... finished successfully","title":"build STAR index"},{"location":"RIMA/build-RIMA-reference/#rseqc-reference-files","text":"We download the human annotation bed file including the whole genome bed file, and house keeping bed file from RSeQC page from sourcforge website . ./ref_files/refseqGenes.bed ./ref_files/housekeeping_refseqGenes.bed","title":"RSeQC reference files"},{"location":"RIMA/build-RIMA-reference/#build-salmon-index","text":"conda activate rna ## salmon Version: salmon 1.1.0 salmon index -t GRCh38.d1.vd1.CIDC.fa -i salmon_index ... index [ \"salmon_index\" ] did not previously exist . . . creating it [ jLog ] [ info ] building index [ jointLog ] [ info ] [ Step 1 of 4 ] : counting k-mers [ jointLog ] [ info ] Replaced 164 ,553,847 non-ATCG nucleotides [ jointLog ] [ info ] Clipped poly-A tails from 0 transcripts [ jointLog ] [ info ] Building rank-select dictionary and saving to disk [ jointLog ] [ info ] done Elapsed time: 0 .191866s [ jointLog ] [ info ] Writing sequence data to file . . . [ jointLog ] [ info ] done Elapsed time: 1 .91244s [ jointLog ] [ info ] Building 64 -bit suffix array ( length of generalized text is 3 ,088,286,426 ) [ jointLog ] [ info ] Building suffix array . . . success saving to disk . . . done Elapsed time: 18 .3072s done Elapsed time: 703 .843s","title":"build salmon index"},{"location":"RIMA/build-RIMA-reference/#gmt-file-for-gene-set-analysis","text":"The GMT file is downloaded from BROAD release page . The current GMT file we used is \"c2.cp.kegg.v6.1.symbols.gmt\"","title":"GMT file for gene set analysis"},{"location":"RIMA/build-RIMA-reference/#star-fusion-genome-resource-lib","text":"The genome resource lib is downloaded from BROAD release page . The current lib we used is GRCh38_v22_CTAT_lib. You can also prep it for use with STAR-fusion. More details, read: https://github.com/STAR-Fusion/STAR-Fusion/wiki/installing-star-fusion","title":"STAR-Fusion genome resource lib"},{"location":"RIMA/build-RIMA-reference/#centrifuge-index","text":"The human Centrifuge index is downloaded from Centrifuge website . The current index we used is p_compressed+h+v that includes human genome, prokaryotic genomes, and viral genomes. You can also build your own custom Centrifuge index. More details, read: https://github.com/DaehwanKimLab/centrifuge","title":"Centrifuge index"},{"location":"RIMA/build-RIMA-reference/#trust4-reference-files","text":"TRUST4 reference files includes 1. TCR, BCR genomic sequence fasta file; 2. Reference database sequence containing annotation information. hg38_bcrtcr.fa human_IMGT+C.fa These reference files can directlt be downloaded from TRUST4 github .","title":"TRUST4 reference files"},{"location":"atac/atac-bwa-reference/","text":"Steps to make bwa indices for ATACseq pipeline \u00b6 Reference fasta \u00b6 The human GDC hg38 fasta file is downloaded from GDC website . The GDC_hg38 fasta file contains random chromosomes and decoy chromosomes and viral genome fasta. e.g., chr11_KI270721v1_random.fa chrUn_GL000195v1.fa chrUn_KN707992v1_decoy.fa HBV.fa HPV100.fa read: https://genestack.com/blog/2016/07/12/choosing-a-reference-genome/ https://lh3.github.io/2017/11/13/which-human-reference-genome-to-use We will only use conventional chromosomes chr1 to chr22 and chrX, chrY. On kraken : cd /liulab/mtang/check_files/GDC_hg38/raw_genome cat chr { 1 ..22 } .fa chrX.fa chrY.fa chrM.fa > ../../GDC_hg38_bwa/GDC_hg38_convential_chrs.fa build bwa indices \u00b6 conda activate chips ## bwa Version: 0.7.15-r1140 cd ../../GDC_hg38_bwa/ bwa index -a bwtsw GDC_hg38_convential_chrs.fa -p GDC_hg38 ... [ BWTIncConstructFromPacked ] 680 iterations done . 6165952882 characters processed. [ bwt_gen ] Finished constructing BWT in 686 iterations. [ bwa_index ] 2257 .53 seconds elapse. [ bwa_index ] Update BWT... 13 .79 sec [ bwa_index ] Pack forward-only FASTA... 10 .69 sec [ bwa_index ] Construct SA from BWT and Occ... 916 .76 sec [ main ] Version: 0 .7.15-r1140 [ main ] CMD: bwa index -a bwtsw -p GDC_hg38 GDC_hg38_convential_chrs.fa [ main ] Real time: 3247 .124 sec ; CPU: 3216 .977 sec","title":"Make bwa indices"},{"location":"atac/atac-bwa-reference/#steps-to-make-bwa-indices-for-atacseq-pipeline","text":"","title":"Steps to make bwa indices for ATACseq pipeline"},{"location":"atac/atac-bwa-reference/#reference-fasta","text":"The human GDC hg38 fasta file is downloaded from GDC website . The GDC_hg38 fasta file contains random chromosomes and decoy chromosomes and viral genome fasta. e.g., chr11_KI270721v1_random.fa chrUn_GL000195v1.fa chrUn_KN707992v1_decoy.fa HBV.fa HPV100.fa read: https://genestack.com/blog/2016/07/12/choosing-a-reference-genome/ https://lh3.github.io/2017/11/13/which-human-reference-genome-to-use We will only use conventional chromosomes chr1 to chr22 and chrX, chrY. On kraken : cd /liulab/mtang/check_files/GDC_hg38/raw_genome cat chr { 1 ..22 } .fa chrX.fa chrY.fa chrM.fa > ../../GDC_hg38_bwa/GDC_hg38_convential_chrs.fa","title":"Reference fasta"},{"location":"atac/atac-bwa-reference/#build-bwa-indices","text":"conda activate chips ## bwa Version: 0.7.15-r1140 cd ../../GDC_hg38_bwa/ bwa index -a bwtsw GDC_hg38_convential_chrs.fa -p GDC_hg38 ... [ BWTIncConstructFromPacked ] 680 iterations done . 6165952882 characters processed. [ bwt_gen ] Finished constructing BWT in 686 iterations. [ bwa_index ] 2257 .53 seconds elapse. [ bwa_index ] Update BWT... 13 .79 sec [ bwa_index ] Pack forward-only FASTA... 10 .69 sec [ bwa_index ] Construct SA from BWT and Occ... 916 .76 sec [ main ] Version: 0 .7.15-r1140 [ main ] CMD: bwa index -a bwtsw -p GDC_hg38 GDC_hg38_convential_chrs.fa [ main ] Real time: 3247 .124 sec ; CPU: 3216 .977 sec","title":"build bwa indices"},{"location":"google-cloud/build-new-image/","text":"How to create an image \u00b6 Author: Aashna, Gali Last modified: 2021-04-02 NOTE: Everytime you make changes in the instance, you may want to store the current version of instance as a new image. Make sure the current instance works before generating a new instance. STOP the instance (STOP, do not delete ) Go to Storage -> Disks. Select the root disk that you want to make an image, i.e \"chips-new-test\" which is 20GB)--click on it an it should bring you to the disk page. On the disk page, select \"Create Image\" from the top Here are the fields you will have to fill: name: name of the image, i.e. chips-ver1-7c source: (do not change it) it should say \"disk\" source disk: (do not change it) location: select regional, and then select us-east1 (south carolina) family: chips description: [add a brief description of what you have added/changed] click add label, then add two labels: key: group, value: plumbers key: pipeline, value: chips then click \"Create\" (at the bottom)","title":"Build Image"},{"location":"google-cloud/build-new-image/#how-to-create-an-image","text":"Author: Aashna, Gali Last modified: 2021-04-02 NOTE: Everytime you make changes in the instance, you may want to store the current version of instance as a new image. Make sure the current instance works before generating a new instance. STOP the instance (STOP, do not delete ) Go to Storage -> Disks. Select the root disk that you want to make an image, i.e \"chips-new-test\" which is 20GB)--click on it an it should bring you to the disk page. On the disk page, select \"Create Image\" from the top Here are the fields you will have to fill: name: name of the image, i.e. chips-ver1-7c source: (do not change it) it should say \"disk\" source disk: (do not change it) location: select regional, and then select us-east1 (south carolina) family: chips description: [add a brief description of what you have added/changed] click add label, then add two labels: key: group, value: plumbers key: pipeline, value: chips then click \"Create\" (at the bottom)","title":"How to create an image"},{"location":"google-cloud/build-reference-snapshot/","text":"How to build reference snapshot \u00b6 Author: Gali Bai Last Modified: 2021-04-01 Step0. Spin up an instance and use the chips-ver1-6 as the boot disk. \u00b6 Log in to the google cloud platform : In the side bar, click VM instanes . On the top, click Create Instance . Give your instance type an unique Name , e.g. chips-test-stanford. Choose Region as us-east1 . Select Machine type : e2-standard-32 . Boot disk: click Change click Custome Images (next to Application Images). select the latest chips image: chips-ver1-6 Firewall: Allow HTTP traffic Allow HTTPs traffic Click Management, security, disks, networking, sole tenancy : click Disks click Add new disk Add new disk for working directory: scroll down to size, and you need to try to predict how much space you need for your analysis, e.g. 2T = 2048. Add new disk for reference: scroll down to size, and here you need to refer how large your reference files are, e.g. chips ref_files is 160G so I assigned 200G. Scroll to the bottom, click Create . NOW you will be brought back to the Google Compute Engine page: click on your new instance when it is up and copy the External IP log in to your instance: ssh -i ~/.ssh/google_cloud_engine galib@ ` External IP ` Step1. Mount Additional disks \u00b6 Format and mount the disks: sudo lsblk #Here you will see two disks sdb and sdc unmounted. sdb is the first disk attached, which is for running chips. sdc is the second attached disk, we will store reference file here later. cd /mnt sudo mkdir chips_refs sudo /home/tain/utils/formatDisk.sh sdc sudo /home/taing/utils/mountDrv.sh sdc /mnt/chip_refs/ Set up reference directory: Copy the most latest version of refernce file you are using to /mnt/chip_refs/ sudo cp -r ~/ref_files /mnt/chip_refs/ #Since we mounted disk sdc to /mnt/chip_refs/, here we are actually copying the reference file to the second disk. Log out the instance: hit CTRL-D and stop the instance (do not delete ). Go to Storage -> Disks . Select the boot disk that you stored the reference file. On the disk page, select Create Snapshot from the top Here are the fields you will have to fill: name: name of the image, i.e. chips-ref-ver1-0 Description: briefly summarize the updates of current reference version source: (do not change it) it should say \"disk\" source disk: (do not change it) location: select regional, and then select us-east1 (South Carolina) description: [add a brief description of what you have added/changed] click add label, then add two labels: key: group , value: plumbers key: pipeline , value: chips then click \"Create\" (at the bottom)","title":"Build Reference Snapshot"},{"location":"google-cloud/build-reference-snapshot/#how-to-build-reference-snapshot","text":"Author: Gali Bai Last Modified: 2021-04-01","title":"How to build reference snapshot"},{"location":"google-cloud/build-reference-snapshot/#step0-spin-up-an-instance-and-use-the-chips-ver1-6-as-the-boot-disk","text":"Log in to the google cloud platform : In the side bar, click VM instanes . On the top, click Create Instance . Give your instance type an unique Name , e.g. chips-test-stanford. Choose Region as us-east1 . Select Machine type : e2-standard-32 . Boot disk: click Change click Custome Images (next to Application Images). select the latest chips image: chips-ver1-6 Firewall: Allow HTTP traffic Allow HTTPs traffic Click Management, security, disks, networking, sole tenancy : click Disks click Add new disk Add new disk for working directory: scroll down to size, and you need to try to predict how much space you need for your analysis, e.g. 2T = 2048. Add new disk for reference: scroll down to size, and here you need to refer how large your reference files are, e.g. chips ref_files is 160G so I assigned 200G. Scroll to the bottom, click Create . NOW you will be brought back to the Google Compute Engine page: click on your new instance when it is up and copy the External IP log in to your instance: ssh -i ~/.ssh/google_cloud_engine galib@ ` External IP `","title":"Step0. Spin up an instance and use the chips-ver1-6 as the boot disk."},{"location":"google-cloud/build-reference-snapshot/#step1-mount-additional-disks","text":"Format and mount the disks: sudo lsblk #Here you will see two disks sdb and sdc unmounted. sdb is the first disk attached, which is for running chips. sdc is the second attached disk, we will store reference file here later. cd /mnt sudo mkdir chips_refs sudo /home/tain/utils/formatDisk.sh sdc sudo /home/taing/utils/mountDrv.sh sdc /mnt/chip_refs/ Set up reference directory: Copy the most latest version of refernce file you are using to /mnt/chip_refs/ sudo cp -r ~/ref_files /mnt/chip_refs/ #Since we mounted disk sdc to /mnt/chip_refs/, here we are actually copying the reference file to the second disk. Log out the instance: hit CTRL-D and stop the instance (do not delete ). Go to Storage -> Disks . Select the boot disk that you stored the reference file. On the disk page, select Create Snapshot from the top Here are the fields you will have to fill: name: name of the image, i.e. chips-ref-ver1-0 Description: briefly summarize the updates of current reference version source: (do not change it) it should say \"disk\" source disk: (do not change it) location: select regional, and then select us-east1 (South Carolina) description: [add a brief description of what you have added/changed] click add label, then add two labels: key: group , value: plumbers key: pipeline , value: chips then click \"Create\" (at the bottom)","title":"Step1. Mount Additional disks"},{"location":"google-cloud/chips-automator/","text":"How to run chips automator \u00b6 Author: Len Taing, Gali Bai Pre-requisites: CIDC Google Cloud account- email James Lindsay software: a. git- read https://git-scm.com/book/en/v2/Getting-Started-Installing-Git b. miniconda3- see how to install miniconda3 below in the appendix PART 0. Set up chips automator conda environment \u00b6 clone the chips automator repository: git clone git@bitbucket.org:plumbers/chips_automator.git NOTE: you only need to do this step once build the chips_automator conda environment and activate it: cd chips_automator conda env create -f chips_automator_env.yml source activate chips_automator NOTE: you only need to do this step once PART 1. Obtaining your Google Cloud Account login key \u00b6 Install the google cloud sdk: conda install -c conda-forge google-cloud-sdk Authenticate your google cloud account: gcloud auth application-default login #And follow the directions NOTE: if successful, there should be a new file called ~/.ssh/google_cloud_engine NOTE: you only need to do this step once a. Copy and paste the link into a browser b. Select your google account c. Click \"Allow\" d. Copy the verification code and paste it into the terminal prompt Set the project: gcloud config set project cidc-biofx Generate your key by logging into an existing google cloud instance: a. create an instance: gcloud compute instances create\u00e5 chips-test-del --machine-type n2-standard-2 --image chips-ver1-7c --service-account biofxvm@cidc-biofx.iam.gserviceaccount.com --scopes https://www.googleapis.com/auth/devstorage.read_write,https://www.googleapis.com/auth/logging.write --zone us-east1-b NOTE: this should return something like this: \"\"\" Created [ https://www.googleapis.com/compute/v1/projects/cidc-biofx/zones/us-east1-b/instances/chips-test-del ]. NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS chips-test-del us-east1-b n2-standard-2 10.142.0.29 35.243.249.170 RUNNING \"\"\" b. Log into instance: gcloud compute ssh chips-test-del #hit Enter and DO NOT enture a passphrase; then Enter again c. generate ssh-keys: ssh-keygen #hit Enter and DO NOT enture a passphrase; then Enter again d. Log out of instance: hit CTRL-D e. download your ssh keys: $ cd ~/Downloads $ scp -i ~/.ssh/google_compute_engine [your username]@[ip addr]:~/.ssh/id_rsa* . f. **EMAIL Len your ssh keys; attach ~/Download/ida_rsa and ida_rsa.pub in the email Before moving on, make sure that Len has added your ssh keys to the chips image; Only proceed if the keys have been added! PART 2. Running chips_automator.py \u00b6 In this section we're going to run the test example that is found in the chips_automator source code. It will create an instance called 'chips-auto-fast-test'. In the chips_automator source code directory, you will find a file called 'test.config.yaml' a. copy over template: NOTE: in this example we're going to name our chips_automator conf file test.config.yaml b. Edit test.config.yaml: instance_name: define the instance name- any arbitrary string but cannon contain '.' cores: define the number of cores for the instance--default is 32 disk_size: define the size of the attached disk google_bucket_path: define the google bucket path to store the results when run is complete chips_ref_snapshot: define which version of chips reference snapshot to use (optional) chips_commit: define the exact chips commit version to use samples: define the sample names and the google bucket paths to their fastqs metasheet: define run name and replicates Run chips automator: ./chips_automator.py -c test.config.yaml -u [ your google cloud username--usually your hostname ] -k ~/.ssh/google_cloud_enging E.g. ./chips_automator.py -c test.config.yaml -u galib -k ~/.ssh/google_cloud_enging #chips automator should run successfully, it will print diagnostic messages until it finishes. If you encounter an error, please send the output of #chips automator to Len. NOTE: -c is where you specify the chips automator config file The final line should look like this- \"\"\" The instance is running at the following IP: 35.243.249.170 please log into this instance and to check-in on the run \"\"\" COPY the IP address for use in the next step Log into the instance to check on the run: ssh -i ~/.ssh/google_cloud_engine [ username ] @ [ ip addr from last line of chips_automator ] cd /mnt/ssd/chips check the state of the chips run by looking at /mnt/ssd/chips/nohup.out PART 3. WHEN the run is complete: \u00b6 NOTE: this is a VERY important part of the process, otherwise the instance is wasting money 1. Delete the instance: a. goto: https://console.cloud.google.com/compute/instances?organizationId=636937865278&project=cidc-biofx&instancessize=50 b. select \"chips-auto-fast-test\" instance and then click the trash can icon at the top of the screen","title":"Run CHIPS automator on gcp"},{"location":"google-cloud/chips-automator/#how-to-run-chips-automator","text":"Author: Len Taing, Gali Bai Pre-requisites: CIDC Google Cloud account- email James Lindsay software: a. git- read https://git-scm.com/book/en/v2/Getting-Started-Installing-Git b. miniconda3- see how to install miniconda3 below in the appendix","title":"How to run chips automator"},{"location":"google-cloud/chips-automator/#part-0-set-up-chips-automator-conda-environment","text":"clone the chips automator repository: git clone git@bitbucket.org:plumbers/chips_automator.git NOTE: you only need to do this step once build the chips_automator conda environment and activate it: cd chips_automator conda env create -f chips_automator_env.yml source activate chips_automator NOTE: you only need to do this step once","title":"PART 0. Set up chips automator conda environment"},{"location":"google-cloud/chips-automator/#part-1-obtaining-your-google-cloud-account-login-key","text":"Install the google cloud sdk: conda install -c conda-forge google-cloud-sdk Authenticate your google cloud account: gcloud auth application-default login #And follow the directions NOTE: if successful, there should be a new file called ~/.ssh/google_cloud_engine NOTE: you only need to do this step once a. Copy and paste the link into a browser b. Select your google account c. Click \"Allow\" d. Copy the verification code and paste it into the terminal prompt Set the project: gcloud config set project cidc-biofx Generate your key by logging into an existing google cloud instance: a. create an instance: gcloud compute instances create\u00e5 chips-test-del --machine-type n2-standard-2 --image chips-ver1-7c --service-account biofxvm@cidc-biofx.iam.gserviceaccount.com --scopes https://www.googleapis.com/auth/devstorage.read_write,https://www.googleapis.com/auth/logging.write --zone us-east1-b NOTE: this should return something like this: \"\"\" Created [ https://www.googleapis.com/compute/v1/projects/cidc-biofx/zones/us-east1-b/instances/chips-test-del ]. NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS chips-test-del us-east1-b n2-standard-2 10.142.0.29 35.243.249.170 RUNNING \"\"\" b. Log into instance: gcloud compute ssh chips-test-del #hit Enter and DO NOT enture a passphrase; then Enter again c. generate ssh-keys: ssh-keygen #hit Enter and DO NOT enture a passphrase; then Enter again d. Log out of instance: hit CTRL-D e. download your ssh keys: $ cd ~/Downloads $ scp -i ~/.ssh/google_compute_engine [your username]@[ip addr]:~/.ssh/id_rsa* . f. **EMAIL Len your ssh keys; attach ~/Download/ida_rsa and ida_rsa.pub in the email Before moving on, make sure that Len has added your ssh keys to the chips image; Only proceed if the keys have been added!","title":"PART 1. Obtaining your Google Cloud Account login key"},{"location":"google-cloud/chips-automator/#part-2-running-chips_automatorpy","text":"In this section we're going to run the test example that is found in the chips_automator source code. It will create an instance called 'chips-auto-fast-test'. In the chips_automator source code directory, you will find a file called 'test.config.yaml' a. copy over template: NOTE: in this example we're going to name our chips_automator conf file test.config.yaml b. Edit test.config.yaml: instance_name: define the instance name- any arbitrary string but cannon contain '.' cores: define the number of cores for the instance--default is 32 disk_size: define the size of the attached disk google_bucket_path: define the google bucket path to store the results when run is complete chips_ref_snapshot: define which version of chips reference snapshot to use (optional) chips_commit: define the exact chips commit version to use samples: define the sample names and the google bucket paths to their fastqs metasheet: define run name and replicates Run chips automator: ./chips_automator.py -c test.config.yaml -u [ your google cloud username--usually your hostname ] -k ~/.ssh/google_cloud_enging E.g. ./chips_automator.py -c test.config.yaml -u galib -k ~/.ssh/google_cloud_enging #chips automator should run successfully, it will print diagnostic messages until it finishes. If you encounter an error, please send the output of #chips automator to Len. NOTE: -c is where you specify the chips automator config file The final line should look like this- \"\"\" The instance is running at the following IP: 35.243.249.170 please log into this instance and to check-in on the run \"\"\" COPY the IP address for use in the next step Log into the instance to check on the run: ssh -i ~/.ssh/google_cloud_engine [ username ] @ [ ip addr from last line of chips_automator ] cd /mnt/ssd/chips check the state of the chips run by looking at /mnt/ssd/chips/nohup.out","title":"PART 2. Running chips_automator.py"},{"location":"google-cloud/chips-automator/#part-3-when-the-run-is-complete","text":"NOTE: this is a VERY important part of the process, otherwise the instance is wasting money 1. Delete the instance: a. goto: https://console.cloud.google.com/compute/instances?organizationId=636937865278&project=cidc-biofx&instancessize=50 b. select \"chips-auto-fast-test\" instance and then click the trash can icon at the top of the screen","title":"PART 3. WHEN the run is complete:"},{"location":"google-cloud/chips-gcp/","text":"How to run chips on Google Cloud \u00b6 Author: Len Taing, Ming Tang, Aashna, Gali Bai Last modified: 02/03/2021 This documentation will guide you through the whole process of running chips pipeline on the google cloud platform. Step0. Spin up an instance and use the chips-ver1-6 as the boot disk. \u00b6 Log in to the google cloud platform : In the side bar, click VM instanes . On the top, click Create Instance . Give your instance type an unique Name , e.g. chips-test-stanford. Choose Region as us-east1 . Select Machine type : e2-standard-32 . Boot disk: click Change click Custome Images (next to Application Images). select the latest chips image: chips-ver1-6 Firewall: Allow HTTP traffic Allow HTTPs traffic Click Management, security, disks, networking, sole tenancy : click Disks click Add new disk or Attach existing disk If Attaching existing disk: select chips-test 200GB If Add new disk: scroll down to size, and you need to try to predict how much space you need for your analysis, e.g. 2T = 2048. Scroll to the bottom, click Create . NOW you will be brought back to the Google Compute Engine page: click on your new instance when it is up and copy the External IP log in to your instance: Step1. Log in to your instance \u00b6 a. If first time logging in: Install a google cloud conda environment wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh # Add and update channels conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge conda install mamba -c conda-forge mamba create -n gcloud -c conda-forge google-cloud-sdk conda activate gcloud gcloud compute ssh chips-test-stanford WHERE: chips-test-stanford is the instance name. NOTE: this creates ~/.ssh/google_compute_engine and ~/.ssh/google_compute._engine.pub b. For old users: After ~/.ssh/google_compute_engine and ~/.ssh/google_compute._engine.pub is created, you can use ssh to log in instead: ssh -i [ your google-cloud-engine key ] [ your username ] @ [ external IP ] e.g. ssh -i ~/.ssh/google_compute_engine galib@XX.YY.ZZ.AA Step2. Formatting and mounting the second drive \u00b6 Check the path of second drive: the additional disk you assigned when spinning up the disk: sudo lsblk Then look for the drive name, e.g. /dev/sdb , that corresponds to the space that you called for. In most cases, it's /dev/sdb . Format the second drive: VERY IMPORTANT: Skip this step if using an old existing disk you only have to format a drive IF it is newly created. IF you are using a drive that was created from before, FORMATTING will delete everything you had from before. So only do this once in the life of a disk! sudo /home/taing/utils/formatDisk.sh [ drive part, e.g. 'sdb' from 2a ] e.g. sudo /home/tain/utils/formatDisk.sh sdb mount the drive: sudo mkdir /mnt/ssd sudo /home/taing/utils/mountDrv.sh [ drive part, e.g. 'sdb' ] /mnt/ [ mount point from above ] e.g. sudo /home/taing/utils/mountDrv.sh sdb /mnt/ssd This will mount /dev/sdb to /mnt/ssd create a directory you can use on /mnt/ssd : sudo mkdir /mnt/ssd/ [ username ] sudo chown [ username ] : [ username ] /mnt/ssd/ [ username ] E.g. sudo mkdir /mnt/ssd/galib sudo chown galib:galib /mnt/ssd/galib NOW you can read and write files to /mnt/ssd/galib without being sudo. REDIRECT /tmp cd /mnt/ssd #or where your newly created disk is sudo mkdir tmp sudo chmod a+w tmp sudo chmod a+r tmp cd / sudo mv tmp/ tmp.bak sudo ln -s /mnt/ssd/tmp Step3. Chips Run \u00b6 cd mnt/ssd/galib/ git clone git@bitbucket.org:plumbers/cidc_chips.git You can follow the documentation on cidc_chips to set up the chips working directory. source /home/taing/miniconda3/bin/activate chips snakemake -s cidc_chips/chips.snakefile -j 16 -np nohup snakemake -s cidc_chips/chips.snakefile -j 16 &","title":"Run CHIPS on gcp"},{"location":"google-cloud/chips-gcp/#how-to-run-chips-on-google-cloud","text":"Author: Len Taing, Ming Tang, Aashna, Gali Bai Last modified: 02/03/2021 This documentation will guide you through the whole process of running chips pipeline on the google cloud platform.","title":"How to run chips on Google Cloud"},{"location":"google-cloud/chips-gcp/#step0-spin-up-an-instance-and-use-the-chips-ver1-6-as-the-boot-disk","text":"Log in to the google cloud platform : In the side bar, click VM instanes . On the top, click Create Instance . Give your instance type an unique Name , e.g. chips-test-stanford. Choose Region as us-east1 . Select Machine type : e2-standard-32 . Boot disk: click Change click Custome Images (next to Application Images). select the latest chips image: chips-ver1-6 Firewall: Allow HTTP traffic Allow HTTPs traffic Click Management, security, disks, networking, sole tenancy : click Disks click Add new disk or Attach existing disk If Attaching existing disk: select chips-test 200GB If Add new disk: scroll down to size, and you need to try to predict how much space you need for your analysis, e.g. 2T = 2048. Scroll to the bottom, click Create . NOW you will be brought back to the Google Compute Engine page: click on your new instance when it is up and copy the External IP log in to your instance:","title":"Step0. Spin up an instance and use the chips-ver1-6 as the boot disk."},{"location":"google-cloud/chips-gcp/#step1-log-in-to-your-instance","text":"a. If first time logging in: Install a google cloud conda environment wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh # Add and update channels conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge conda install mamba -c conda-forge mamba create -n gcloud -c conda-forge google-cloud-sdk conda activate gcloud gcloud compute ssh chips-test-stanford WHERE: chips-test-stanford is the instance name. NOTE: this creates ~/.ssh/google_compute_engine and ~/.ssh/google_compute._engine.pub b. For old users: After ~/.ssh/google_compute_engine and ~/.ssh/google_compute._engine.pub is created, you can use ssh to log in instead: ssh -i [ your google-cloud-engine key ] [ your username ] @ [ external IP ] e.g. ssh -i ~/.ssh/google_compute_engine galib@XX.YY.ZZ.AA","title":"Step1. Log in to your instance"},{"location":"google-cloud/chips-gcp/#step2-formatting-and-mounting-the-second-drive","text":"Check the path of second drive: the additional disk you assigned when spinning up the disk: sudo lsblk Then look for the drive name, e.g. /dev/sdb , that corresponds to the space that you called for. In most cases, it's /dev/sdb . Format the second drive: VERY IMPORTANT: Skip this step if using an old existing disk you only have to format a drive IF it is newly created. IF you are using a drive that was created from before, FORMATTING will delete everything you had from before. So only do this once in the life of a disk! sudo /home/taing/utils/formatDisk.sh [ drive part, e.g. 'sdb' from 2a ] e.g. sudo /home/tain/utils/formatDisk.sh sdb mount the drive: sudo mkdir /mnt/ssd sudo /home/taing/utils/mountDrv.sh [ drive part, e.g. 'sdb' ] /mnt/ [ mount point from above ] e.g. sudo /home/taing/utils/mountDrv.sh sdb /mnt/ssd This will mount /dev/sdb to /mnt/ssd create a directory you can use on /mnt/ssd : sudo mkdir /mnt/ssd/ [ username ] sudo chown [ username ] : [ username ] /mnt/ssd/ [ username ] E.g. sudo mkdir /mnt/ssd/galib sudo chown galib:galib /mnt/ssd/galib NOW you can read and write files to /mnt/ssd/galib without being sudo. REDIRECT /tmp cd /mnt/ssd #or where your newly created disk is sudo mkdir tmp sudo chmod a+w tmp sudo chmod a+r tmp cd / sudo mv tmp/ tmp.bak sudo ln -s /mnt/ssd/tmp","title":"Step2. Formatting and mounting the second drive"},{"location":"google-cloud/chips-gcp/#step3-chips-run","text":"cd mnt/ssd/galib/ git clone git@bitbucket.org:plumbers/cidc_chips.git You can follow the documentation on cidc_chips to set up the chips working directory. source /home/taing/miniconda3/bin/activate chips snakemake -s cidc_chips/chips.snakefile -j 16 -np nohup snakemake -s cidc_chips/chips.snakefile -j 16 &","title":"Step3. Chips Run"},{"location":"google-cloud/common-questions/","text":"Frequently Asked Questions \u00b6 Q1. How to update conda environment under /home/taing/miniconda? \u00b6 You can become any other user on GCP by typing: $ sudo -su [ username ] For example you can become user taing and edit the conda environment like this: switch user sudo -su taing 2. Source taing's miniconda environment make the following taing_env.bash touch taing_env.bash nano taing_env.bash Copy the following lines in taing_env.bash export CONDA_ROOT = /home/taing/miniconda3 export CONDA_ENVS_PATH = $CONDA_ROOT /envs export PATH = /home/taing/miniconda3/bin: $PATH unset PYTHONPATH export HOME = /home/taing source taing_env.bash 3. Add tools E.g. You want to add fastp in current conda env: install fastp conda activate chips conda install fastp -c bioconda Ctrl + D will switch back to your own user. Remember to create a new image based on updated instance, otherwise your update will not be saved. Q2. Automator: Sending command gsutil remotely from your computer to the instance causing error message bash: gsutil: command not found ? \u00b6 Log in to the instance $ which gsutil # if there is no path under /usr/local/bin: $ ln -s ~/gsutil /usr/local/bin/ Create a new image for the current updated instance. Q3. Permission denied for running sentieon with error messages /home/taing/utils/setup03_sentieonLic.sh: line 6: /home/taing/nohup.sentieon.out: Permission denied or \u00b6 Failed to contact the license server at gcp.sentieon.com:9003 . Go to /home/taing/sentieon Switch user to taing. sudo su taing Open permissions to the sentieon and related files. chmod 777 /home/taing/sentieon Create a new image for the current updated instance.","title":"Common Q&A"},{"location":"google-cloud/common-questions/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"google-cloud/common-questions/#q1-how-to-update-conda-environment-under-hometaingminiconda","text":"You can become any other user on GCP by typing: $ sudo -su [ username ] For example you can become user taing and edit the conda environment like this: switch user sudo -su taing 2. Source taing's miniconda environment make the following taing_env.bash touch taing_env.bash nano taing_env.bash Copy the following lines in taing_env.bash export CONDA_ROOT = /home/taing/miniconda3 export CONDA_ENVS_PATH = $CONDA_ROOT /envs export PATH = /home/taing/miniconda3/bin: $PATH unset PYTHONPATH export HOME = /home/taing source taing_env.bash 3. Add tools E.g. You want to add fastp in current conda env: install fastp conda activate chips conda install fastp -c bioconda Ctrl + D will switch back to your own user. Remember to create a new image based on updated instance, otherwise your update will not be saved.","title":"Q1. How to update conda environment under /home/taing/miniconda?"},{"location":"google-cloud/common-questions/#q2-automator-sending-command-gsutil-remotely-from-your-computer-to-the-instance-causing-error-message-bash-gsutil-command-not-found","text":"Log in to the instance $ which gsutil # if there is no path under /usr/local/bin: $ ln -s ~/gsutil /usr/local/bin/ Create a new image for the current updated instance.","title":"Q2. Automator: Sending command gsutil remotely from your computer to the instance causing error message bash: gsutil: command not found?"},{"location":"google-cloud/common-questions/#q3-permission-denied-for-running-sentieon-with-error-messages-hometaingutilssetup03_sentieonlicsh-line-6-hometaingnohupsentieonout-permission-denied-or","text":"Failed to contact the license server at gcp.sentieon.com:9003 . Go to /home/taing/sentieon Switch user to taing. sudo su taing Open permissions to the sentieon and related files. chmod 777 /home/taing/sentieon Create a new image for the current updated instance.","title":"Q3. Permission denied for running sentieon with error messages /home/taing/utils/setup03_sentieonLic.sh: line 6: /home/taing/nohup.sentieon.out: Permission denied or"},{"location":"google-cloud/expand-partition/","text":"Expand root partition \u00b6 Sometimes you want to increase the root parition (booting disk) after you created an instance. Len showed me how to do it. I used his chips-1-4 image as a base (20G in size), and want to increase the root partition size. To expand the root partition: \u00b6 On the GCP webpage, click Disks (Sidebar) and click the root disk of the instance--usually named the same as the instance Click the edit icon and change the size (in GB) to the desired size on the instance: # and look for the device name, usually sda1 $ lsblk # Grow the partition: $ sudo growpart /dev/sda1 #resize the fs: $ sudo resize2fs /dev/sda1 # to ensure everything worked $ df -h","title":"Expand root partition"},{"location":"google-cloud/expand-partition/#expand-root-partition","text":"Sometimes you want to increase the root parition (booting disk) after you created an instance. Len showed me how to do it. I used his chips-1-4 image as a base (20G in size), and want to increase the root partition size.","title":"Expand root partition"},{"location":"google-cloud/expand-partition/#to-expand-the-root-partition","text":"On the GCP webpage, click Disks (Sidebar) and click the root disk of the instance--usually named the same as the instance Click the edit icon and change the size (in GB) to the desired size on the instance: # and look for the device name, usually sda1 $ lsblk # Grow the partition: $ sudo growpart /dev/sda1 #resize the fs: $ sudo resize2fs /dev/sda1 # to ensure everything worked $ df -h","title":"To expand the root partition:"},{"location":"google-cloud/tcr-gcp/","text":"TCR ANALYSIS ON GCP \u00b6 INITIAL SETTING UP \u00b6 VM properties recommendation: \u00b6 n1-standard-1 (1 vCPU, 3.75 GB memory) Disk properties recommendation: \u00b6 Debian 10 image, 10GB storage R, R package versions: \u00b6 These exact version numbers are by no means essential. They have simply been tested on Google Cloud previously. R version 3.5.2 data.table 1.12.8 immunarch 0.6.5 naturalsort 0.1.3 dplyr 1.0.0 rjson 0.2.20 Additional info for running VisualizIRR: \u00b6 https://github.com/d-s-cohen/visualizirr 1. Setting up disk: \u00b6 # Set up TCR dir sudo mkdir /tcr/ sudo chmod 777 -R /tcr/ # Install R sudo apt-get update sudo apt-get install r-base r-base-dev libtool pkg-config libnlopt-dev libcurl4-openssl-dev # Get VisualizIRR files cd /tcr/ curl 'https://github.com/d-s-cohen/visualizirr/archive/refs/heads/master.zip' --output master.zip unzip master.zip rm master.zip mv * template 2. Change values in template/r/config.R: \u00b6 input_format value (Likely \"ADAPTIVE\" or \"RHTCRSEQ\") json_out=TRUE 3. Install R packages as root: \u00b6 These can be installed automatically when VisualizIRR runs but it is best to install them ahead of time. list.of.packages <- c ( \"data.table\" , \"immunarch\" , \"naturalsort\" , \"dplyr\" , \"rjson\" , \"BiocManager\" ) new.packages <- list.of.packages[ ! ( list.of.packages %in% installed.packages () [ , \"Package\" ] ) ] install.packages ( new.packages , repos = \"http://cran.us.r-project.org\" ) BiocManager :: install ( \"Biostrings\" ) # Test loading packages suppressMessages ( library ( data.table )) suppressMessages ( library ( immunarch )) suppressMessages ( library ( naturalsort )) suppressMessages ( library ( dplyr )) suppressMessages ( library ( rjson )) suppressMessages ( library ( BiocManager ) suppressMessages ( library ( Biostrings )) NOTE: If you have trouble with this section, relating to the pbkrtest package, try: sudo Rscript -e 'install.packages(\"https://cran.r-project.org/src/contrib/Archive/pbkrtest/pbkrtest_0.4-7.tar.gz\")' 4. Add automator script as /tcr/automator.sh : \u00b6 #!/bin/bash set -x cp -r /tcr/template $1 _report echo \"input_dir = ' $1 _data'\" >> $1 _report/r/config.R echo \"output_dir = ' $1 _report/data'\" >> $1 _report/r/config.R echo \"output_name = ' $1 '\" >> $1 _report/r/config.R echo \"report_dir = ' $1 _report'\" >> $1 _report/r/config.R Rscript $1 _report/r/immuneRepProcess.R $1 _report/r/config.R cp $1 _meta.csv $1 _report/data/meta.csv tar -czvf $1 _report.tar.gz $1 _report 5. File permissions: \u00b6 chmod u+x /tcr/automator.sh RUNNING ANALYSIS \u00b6 Make sure directory containing repertoire files is located as /tcr/<cohort_name>_data/ Make sure metadata file is located as /tcr/<cohort_name>_meta.csv You may also organize all work into /tcr/ subdirectories if you wish (EX: /tcr/cohort1/, /tcr/cohort2/) Command for running VisualizIRR using the automator script: cd /tcr/ nohup ./automator.sh <cohort_name> > <cohort_name>.log & Report output located at /tcr/<cohort_name>_report.tar.gz","title":"Run TCR on gcp"},{"location":"google-cloud/tcr-gcp/#tcr-analysis-on-gcp","text":"","title":"TCR ANALYSIS ON GCP"},{"location":"google-cloud/tcr-gcp/#initial-setting-up","text":"","title":"INITIAL SETTING UP"},{"location":"google-cloud/tcr-gcp/#vm-properties-recommendation","text":"n1-standard-1 (1 vCPU, 3.75 GB memory)","title":"VM properties recommendation:"},{"location":"google-cloud/tcr-gcp/#disk-properties-recommendation","text":"Debian 10 image, 10GB storage","title":"Disk properties recommendation:"},{"location":"google-cloud/tcr-gcp/#r-r-package-versions","text":"These exact version numbers are by no means essential. They have simply been tested on Google Cloud previously. R version 3.5.2 data.table 1.12.8 immunarch 0.6.5 naturalsort 0.1.3 dplyr 1.0.0 rjson 0.2.20","title":"R, R package versions:"},{"location":"google-cloud/tcr-gcp/#additional-info-for-running-visualizirr","text":"https://github.com/d-s-cohen/visualizirr","title":"Additional info for running VisualizIRR:"},{"location":"google-cloud/tcr-gcp/#1-setting-up-disk","text":"# Set up TCR dir sudo mkdir /tcr/ sudo chmod 777 -R /tcr/ # Install R sudo apt-get update sudo apt-get install r-base r-base-dev libtool pkg-config libnlopt-dev libcurl4-openssl-dev # Get VisualizIRR files cd /tcr/ curl 'https://github.com/d-s-cohen/visualizirr/archive/refs/heads/master.zip' --output master.zip unzip master.zip rm master.zip mv * template","title":"1. Setting up disk:"},{"location":"google-cloud/tcr-gcp/#2-change-values-in-templaterconfigr","text":"input_format value (Likely \"ADAPTIVE\" or \"RHTCRSEQ\") json_out=TRUE","title":"2. Change values in template/r/config.R:"},{"location":"google-cloud/tcr-gcp/#3-install-r-packages-as-root","text":"These can be installed automatically when VisualizIRR runs but it is best to install them ahead of time. list.of.packages <- c ( \"data.table\" , \"immunarch\" , \"naturalsort\" , \"dplyr\" , \"rjson\" , \"BiocManager\" ) new.packages <- list.of.packages[ ! ( list.of.packages %in% installed.packages () [ , \"Package\" ] ) ] install.packages ( new.packages , repos = \"http://cran.us.r-project.org\" ) BiocManager :: install ( \"Biostrings\" ) # Test loading packages suppressMessages ( library ( data.table )) suppressMessages ( library ( immunarch )) suppressMessages ( library ( naturalsort )) suppressMessages ( library ( dplyr )) suppressMessages ( library ( rjson )) suppressMessages ( library ( BiocManager ) suppressMessages ( library ( Biostrings )) NOTE: If you have trouble with this section, relating to the pbkrtest package, try: sudo Rscript -e 'install.packages(\"https://cran.r-project.org/src/contrib/Archive/pbkrtest/pbkrtest_0.4-7.tar.gz\")'","title":"3. Install R packages as root:"},{"location":"google-cloud/tcr-gcp/#4-add-automator-script-as-tcrautomatorsh","text":"#!/bin/bash set -x cp -r /tcr/template $1 _report echo \"input_dir = ' $1 _data'\" >> $1 _report/r/config.R echo \"output_dir = ' $1 _report/data'\" >> $1 _report/r/config.R echo \"output_name = ' $1 '\" >> $1 _report/r/config.R echo \"report_dir = ' $1 _report'\" >> $1 _report/r/config.R Rscript $1 _report/r/immuneRepProcess.R $1 _report/r/config.R cp $1 _meta.csv $1 _report/data/meta.csv tar -czvf $1 _report.tar.gz $1 _report","title":"4. Add automator script as /tcr/automator.sh:"},{"location":"google-cloud/tcr-gcp/#5-file-permissions","text":"chmod u+x /tcr/automator.sh","title":"5. File permissions:"},{"location":"google-cloud/tcr-gcp/#running-analysis","text":"Make sure directory containing repertoire files is located as /tcr/<cohort_name>_data/ Make sure metadata file is located as /tcr/<cohort_name>_meta.csv You may also organize all work into /tcr/ subdirectories if you wish (EX: /tcr/cohort1/, /tcr/cohort2/) Command for running VisualizIRR using the automator script: cd /tcr/ nohup ./automator.sh <cohort_name> > <cohort_name>.log & Report output located at /tcr/<cohort_name>_report.tar.gz","title":"RUNNING ANALYSIS"},{"location":"google-cloud/wes-gcp/","text":"How to run WES pipeline in google cloud \u00b6 Thanks Len and Aashna for sharing it. how to get your google cloud credentials \u00b6 a. install the gcloud sdk: (it's on conda) conda install -c conda-forge google-cloud-sdk b. Authenticate: gcloud auth login NOTE: opens up a webbrowser, where I need to select the google account to allow access c. set project: gcloud config set project cidc-biofx CREATE the new instance: \u00b6 a. Goto google cloud platform : Google Compute Engine -> click \"Create Instance\" give your instance type a unique name, e.g. wes-aashna-1 Machine type: select high-mem-64 (this has 64 cores) Boot disk: click \"Change\". click \"Custome Images\" (next to Application Images). select latest wes image-. AS OF 2019-02-04, it is wes-ver-1-1b Click \"Management, security, disks, networking, sole tenancy\". click Disks click \"Add new disk\" scroll down to size, and HERE you need to try to predict how much space you need to do your analysis e.g. 2T = 2048 scroll to the bottom, click \"Create\" NOW you will be brough back to the Google Compute Engine page click on your new instance when it is up and get the IP address login to your instance: gcloud compute ssh test-instance-1 WHERE test-instance-1 is the instance name. NOTE: this creates ~/.ssh/google_compute_engine and ~/.ssh/google_compute._engine.pub **NOTE: You only need to do this once. after ~/.ssh/google_compute_engine and ~/.ssh/google_compute._engine.pub is created, you can use this ssh cmd to log in instead: ssh -i [ your google-cloud-engine key ] [ your username ] @ [ ipaddress in 1b. ] e.g. ssh -i ~/.ssh/google_compute_engine aashna@XX.YY.ZZ.AA Please read Connecting to Linux instances and Connecting to instances using advanced methods for more details. formatting and mounting the second drive- \u00b6 After logging in, the next step is to mount and format the drive that you apportioned in step 1.a.4 so that you can use it. a. finding the disk name: type sudo lsblk Then look for the drive name, e.g. /dev/sdb , that corresponds to the space that you called for. In most cases, it's /dev/sdb b. format the drive: type: VERY IMPORTANT-- PLEASE READ!!!! you only have to format a drive IF it is newly created. IF you are using a drive that was created from before, FORMATTING will delete everything you had from before. SO only do this once in the life of a disk! IF using an old disk, skip this step! sudo /home/taing/utils/formatDisk.sh [ drive part, e.g. 'sdb' from 2a ] e.g. sudo /home/taing/utils/formatDisk.sh sdb c. mount the drive: make a mount directory: EXAMPLE: sudo mkdir /mnt/ssd mount the drive: sudo /home/taing/utils/mountDrv.sh [ drive part, e.g. 'sdb' ] /mnt/ [ mount point from above ] Example: sudo /home/taing/utils/mountDrv.sh sdb /mnt/ssd This will mount /dev/sdb to /mnt/ssd d. create a directory you can use on /mnt/ssd : sudo mkdir /mnt/ssd/ [ username ] sudo chown [ username ] : [ username ] /mnt/ssd/ [ username ] EXAMPLE: sudo mkdir /mnt/ssd/aashna sudo chown aashna:aashna /mnt/ssd/aashna NOW you can read and write files to /mnt/ssd/aashan without being sudo e. REDIRECT /tmp NOTE: sometimes the /tmp directory can get full. To ensure this doesn't happen, I usually move /tmp off of the root partition cd /mnt/ssd #or where your newly created disk is sudo mkdir tmp sudo chmod a+w tmp sudo chmod a+r tmp soft-link cd / sudo mv tmp/ tmp.bak sudo ln -s /mnt/ssd/tmp Steps for WES setup \u00b6 f. run sentieon license: cd /home/taing/utils/ nohup ./sentieonLicense.sh & how to setup a wes run: (in the directory created in 2d.) \u00b6 change into your directory from 2d: cd /mnt/ssd/ [ username ] # clone the wes repository: git clone https://AashnaJhaveri@bitbucket.org/plumbers/cidc_wes.git # create a data directory: mkdir data # upload your fastqs into data copy out the config.yaml and metasheet.csv: cp cidc_wes/config.yaml . cp cidc_wes/metasheet.csv . Edit the config.yaml to fill the sentieon path which I believe is something: /home/taing/sentieon/sentieon.../bin fill the samples section- #Jingxin, ask aashna about this Edit metasheet.csv to define the Normal/Tumor pairs link the reference files: The reference files include, for example, the bwa index and the genome's FASTA file. ln -s /mnt/cidc_nfs/wes/ref_files Now you are ready to run RUN wes: \u00b6 source activate wes Your cmd line should be pre-pended with (wes) do a dry run to check for errors in config or metasheet snakemake -s cidc_wes/wes.snakefile -n If all is green you are good to go If there are errors, fix them FULL run: nohup time snakemake -s cidc_wes/wes.snakefile -j 64 > nohup.out & The 'nohup' allows you to log off. The 'time' will time the run for you. the -j 64 means to use 64. Use whatever number you want.","title":"Run WES on gcp"},{"location":"google-cloud/wes-gcp/#how-to-run-wes-pipeline-in-google-cloud","text":"Thanks Len and Aashna for sharing it.","title":"How to run WES pipeline in google cloud"},{"location":"google-cloud/wes-gcp/#how-to-get-your-google-cloud-credentials","text":"a. install the gcloud sdk: (it's on conda) conda install -c conda-forge google-cloud-sdk b. Authenticate: gcloud auth login NOTE: opens up a webbrowser, where I need to select the google account to allow access c. set project: gcloud config set project cidc-biofx","title":"how to get your google cloud credentials"},{"location":"google-cloud/wes-gcp/#create-the-new-instance","text":"a. Goto google cloud platform : Google Compute Engine -> click \"Create Instance\" give your instance type a unique name, e.g. wes-aashna-1 Machine type: select high-mem-64 (this has 64 cores) Boot disk: click \"Change\". click \"Custome Images\" (next to Application Images). select latest wes image-. AS OF 2019-02-04, it is wes-ver-1-1b Click \"Management, security, disks, networking, sole tenancy\". click Disks click \"Add new disk\" scroll down to size, and HERE you need to try to predict how much space you need to do your analysis e.g. 2T = 2048 scroll to the bottom, click \"Create\" NOW you will be brough back to the Google Compute Engine page click on your new instance when it is up and get the IP address login to your instance: gcloud compute ssh test-instance-1 WHERE test-instance-1 is the instance name. NOTE: this creates ~/.ssh/google_compute_engine and ~/.ssh/google_compute._engine.pub **NOTE: You only need to do this once. after ~/.ssh/google_compute_engine and ~/.ssh/google_compute._engine.pub is created, you can use this ssh cmd to log in instead: ssh -i [ your google-cloud-engine key ] [ your username ] @ [ ipaddress in 1b. ] e.g. ssh -i ~/.ssh/google_compute_engine aashna@XX.YY.ZZ.AA Please read Connecting to Linux instances and Connecting to instances using advanced methods for more details.","title":"CREATE the new instance:"},{"location":"google-cloud/wes-gcp/#formatting-and-mounting-the-second-drive-","text":"After logging in, the next step is to mount and format the drive that you apportioned in step 1.a.4 so that you can use it. a. finding the disk name: type sudo lsblk Then look for the drive name, e.g. /dev/sdb , that corresponds to the space that you called for. In most cases, it's /dev/sdb b. format the drive: type: VERY IMPORTANT-- PLEASE READ!!!! you only have to format a drive IF it is newly created. IF you are using a drive that was created from before, FORMATTING will delete everything you had from before. SO only do this once in the life of a disk! IF using an old disk, skip this step! sudo /home/taing/utils/formatDisk.sh [ drive part, e.g. 'sdb' from 2a ] e.g. sudo /home/taing/utils/formatDisk.sh sdb c. mount the drive: make a mount directory: EXAMPLE: sudo mkdir /mnt/ssd mount the drive: sudo /home/taing/utils/mountDrv.sh [ drive part, e.g. 'sdb' ] /mnt/ [ mount point from above ] Example: sudo /home/taing/utils/mountDrv.sh sdb /mnt/ssd This will mount /dev/sdb to /mnt/ssd d. create a directory you can use on /mnt/ssd : sudo mkdir /mnt/ssd/ [ username ] sudo chown [ username ] : [ username ] /mnt/ssd/ [ username ] EXAMPLE: sudo mkdir /mnt/ssd/aashna sudo chown aashna:aashna /mnt/ssd/aashna NOW you can read and write files to /mnt/ssd/aashan without being sudo e. REDIRECT /tmp NOTE: sometimes the /tmp directory can get full. To ensure this doesn't happen, I usually move /tmp off of the root partition cd /mnt/ssd #or where your newly created disk is sudo mkdir tmp sudo chmod a+w tmp sudo chmod a+r tmp soft-link cd / sudo mv tmp/ tmp.bak sudo ln -s /mnt/ssd/tmp","title":"formatting and mounting the second drive-"},{"location":"google-cloud/wes-gcp/#steps-for-wes-setup","text":"f. run sentieon license: cd /home/taing/utils/ nohup ./sentieonLicense.sh &","title":"Steps for WES setup"},{"location":"google-cloud/wes-gcp/#how-to-setup-a-wes-run-in-the-directory-created-in-2d","text":"change into your directory from 2d: cd /mnt/ssd/ [ username ] # clone the wes repository: git clone https://AashnaJhaveri@bitbucket.org/plumbers/cidc_wes.git # create a data directory: mkdir data # upload your fastqs into data copy out the config.yaml and metasheet.csv: cp cidc_wes/config.yaml . cp cidc_wes/metasheet.csv . Edit the config.yaml to fill the sentieon path which I believe is something: /home/taing/sentieon/sentieon.../bin fill the samples section- #Jingxin, ask aashna about this Edit metasheet.csv to define the Normal/Tumor pairs link the reference files: The reference files include, for example, the bwa index and the genome's FASTA file. ln -s /mnt/cidc_nfs/wes/ref_files Now you are ready to run","title":"how to setup a wes run: (in the directory created in 2d.)"},{"location":"google-cloud/wes-gcp/#run-wes","text":"source activate wes Your cmd line should be pre-pended with (wes) do a dry run to check for errors in config or metasheet snakemake -s cidc_wes/wes.snakefile -n If all is green you are good to go If there are errors, fix them FULL run: nohup time snakemake -s cidc_wes/wes.snakefile -j 64 > nohup.out & The 'nohup' allows you to log off. The 'time' will time the run for you. the -j 64 means to use 64. Use whatever number you want.","title":"RUN wes:"},{"location":"google-cloud/wes-ingestion/","text":"CIDC Software team ingestion script how-to \u00b6 By Len Taing 2020-05-18 ref: https://stagingportal.cimac-network.org/analyses//cli-instructions Create a conda environment for the cidc-cli script: \u00b6 NOTE: wes images >= wes-ver1-99c already have the cidc-cli conda env located at: /home/taing/miniconda3/envs/cidc-cli so there's no need to build a new one for wes google images a. write a file called cidc-cli.env.yml with the following content: name: cidc-cli channels: - bioconda - defaults dependencies: - python=3 - pip: - cidc-cli b. create the conda env: conda env create -f cidc-cli.env.yml -n cidc-cli c. When done, save a new google image, e.g. for WES I created wes-ver1-99c d. start a new google image with the image you just created FROM this point on in the how-to, it is assumed that the cidc-cli conda env and scripts are installed correctly. The workflow from the software team is: 1. the software team will generate a automator config file (e.g. wes_automator config file) and a corresponding .xslx sheet for a patient/batch of samples to RUN and ingest 2. The bioinformatics team (that's us) will run the automator config file given in step 1. This how to ALSO assumes that the pipeline run from the automator ran successfully and it is now time to ingest results into the cidc software team's db AFTER the wes/rnaseq automator run is complete (without errors) \u00b6 a. activate the cidc-cli conda env: source activate cidc-cli b. UPLOAD the .xlsx sheet associated with the automator config (given by the software team) to the instance: NOTE: in this example, the .xlsx sheet is called \"wes_analysis_CTTTP02N1.00_20200129.xlsx\" EXAMPLE: scp -i ~/.ssh/google_compute_engine wes_analysis_CTTTP02N1.00_20200129.xlsx taing@35.237.164.38:/mnt/ssd/wes c. LOG in: 1. goto: https://stagingportal.cimac-network.org/analyses//cli-instructions 2. scroll to the bottom of page and copy login cmd (i.e. last line): cidc login ...[long key string]... 3. paste cmd into ssh shell d. RUN ingestion script: cidc analyses upload --analysis wes_analysis --xlsx ./wes_analysis_CTTTP01N1.00_20200129.xlsx NOTE: the script my prompt you with questions- answer \"Yes\" to all of them; for the google auth step, copy and paste the URL given and complete the authentication process. NOTE: the parameter --analysis may be different depending on your pipeline NOTE: the ingestion script will now try to upload the files and it may take some time","title":"WES ingestion on gcp"},{"location":"google-cloud/wes-ingestion/#cidc-software-team-ingestion-script-how-to","text":"By Len Taing 2020-05-18 ref: https://stagingportal.cimac-network.org/analyses//cli-instructions","title":"CIDC Software team ingestion script how-to"},{"location":"google-cloud/wes-ingestion/#create-a-conda-environment-for-the-cidc-cli-script","text":"NOTE: wes images >= wes-ver1-99c already have the cidc-cli conda env located at: /home/taing/miniconda3/envs/cidc-cli so there's no need to build a new one for wes google images a. write a file called cidc-cli.env.yml with the following content: name: cidc-cli channels: - bioconda - defaults dependencies: - python=3 - pip: - cidc-cli b. create the conda env: conda env create -f cidc-cli.env.yml -n cidc-cli c. When done, save a new google image, e.g. for WES I created wes-ver1-99c d. start a new google image with the image you just created FROM this point on in the how-to, it is assumed that the cidc-cli conda env and scripts are installed correctly. The workflow from the software team is: 1. the software team will generate a automator config file (e.g. wes_automator config file) and a corresponding .xslx sheet for a patient/batch of samples to RUN and ingest 2. The bioinformatics team (that's us) will run the automator config file given in step 1. This how to ALSO assumes that the pipeline run from the automator ran successfully and it is now time to ingest results into the cidc software team's db","title":"Create a conda environment for the cidc-cli script:"},{"location":"google-cloud/wes-ingestion/#after-the-wesrnaseq-automator-run-is-complete-without-errors","text":"a. activate the cidc-cli conda env: source activate cidc-cli b. UPLOAD the .xlsx sheet associated with the automator config (given by the software team) to the instance: NOTE: in this example, the .xlsx sheet is called \"wes_analysis_CTTTP02N1.00_20200129.xlsx\" EXAMPLE: scp -i ~/.ssh/google_compute_engine wes_analysis_CTTTP02N1.00_20200129.xlsx taing@35.237.164.38:/mnt/ssd/wes c. LOG in: 1. goto: https://stagingportal.cimac-network.org/analyses//cli-instructions 2. scroll to the bottom of page and copy login cmd (i.e. last line): cidc login ...[long key string]... 3. paste cmd into ssh shell d. RUN ingestion script: cidc analyses upload --analysis wes_analysis --xlsx ./wes_analysis_CTTTP01N1.00_20200129.xlsx NOTE: the script my prompt you with questions- answer \"Yes\" to all of them; for the google auth step, copy and paste the URL given and complete the authentication process. NOTE: the parameter --analysis may be different depending on your pipeline NOTE: the ingestion script will now try to upload the files and it may take some time","title":"AFTER the wes/rnaseq automator run is complete (without errors)"},{"location":"kraken/kraken_jupyter/","text":"Run Juypter Notebook on the remote cluster \u00b6 Run Juypter Notebook on the remote cluster \u00b6 The note is taken from https://docs.rc.fas.harvard.edu/kb/jupyter-notebook-server-on-cluster/ start a job on the remote server \u00b6 require an interactive node $ ssh kraken # time in mins $ srun -t 1600 --mem = 20G -c 4 --pty bash # use mamba to replace conda for faster installation on kraken $ mamba create -n jupyter_3.6 python = 3 .6 jupyter ## you can add any other package to install you might need. $ source activate jupyter_3.6 To help you connect to the Jupyter server, run the following command to get the hostname. Select an available port between 6818 and 11845 (in this example, 6820 is the first such available port): $ for myport in { 6818 ..11845 } ; do ! nc -z localhost ${ myport } && break ; done $ echo \"ssh -NL $myport : $( hostname ) : $myport $USER @kraken.dfci.harvard.edu\" # ssh -NL 6819:node21:6819 mtang@kraken.dfci.harvard.edu Starting teh ntoebook server: jupyter-notebook --no-browser --port = $myport --ip = '0.0.0.0' on your local computer \u00b6 ssh -NL 6819 :node21:6819 mtang@kraken.dfci.harvard.edu Then in your workstation/laptop browser. Make sure to copy the token from the Jupyter notebook server and update the token below. http://localhost:6819/?token=<TokenFromYourServer> Run Juypter Notebook on the remote cluster in a container \u00b6 The note is from https://github.com/nteract/hydrogen/issues/1184 I am running in a Singularity container on a remote cluster, working on node called 'node01'. on the Server \u00b6 edit ~/.jupyter/jupyter_notebook_config.py, commenting in c.NotebookApp.token = 'my_new_token_that_I_wrote' run jupyter notebook, here for my install: singularity exec -B /path/to/my/jupyter_notebook:/home/bmoran/jupyter_notebook /path/to/singularity/containter.for.analysis.sif jupyter-notebook --no-browser --port = 8891 --ip = 127 .0.0.1 on your Local machine \u00b6 ssh tunnel to server ssh -N -f -L 127.0.0.1:8891:127.0.0.1:8891 node01 -v -v you should be able to copy/paste the URL printed to screen on server and connect locally in browser now. Atom \u00b6 Packages -> Settings -> Manage Packages -> Hydrogen Settings [[scroll to Kernel Gateways, adding following]] [{ \"name\": \"node01\", \"options\": { \"baseUrl\": \" http://127.0.0.1:8891 \", \"token\": \"my_new_token_that_I_wrote\" } }] and restart Atom Open a file in Atom that you want to run in the kernel that you are connected to in browser Packages -> Hydrogen -> Connect to Remote Kernel [[select a gateway, click node01, enter token]] The filename open in the browser should be available to select, and run in kernel, edit and save in your current script. NB that any script in Atom can now be run in kernel...very nice!","title":"Run jupyter notebook"},{"location":"kraken/kraken_jupyter/#run-juypter-notebook-on-the-remote-cluster","text":"","title":"Run Juypter Notebook on the remote cluster"},{"location":"kraken/kraken_jupyter/#run-juypter-notebook-on-the-remote-cluster_1","text":"The note is taken from https://docs.rc.fas.harvard.edu/kb/jupyter-notebook-server-on-cluster/","title":"Run Juypter Notebook on the remote cluster"},{"location":"kraken/kraken_jupyter/#start-a-job-on-the-remote-server","text":"require an interactive node $ ssh kraken # time in mins $ srun -t 1600 --mem = 20G -c 4 --pty bash # use mamba to replace conda for faster installation on kraken $ mamba create -n jupyter_3.6 python = 3 .6 jupyter ## you can add any other package to install you might need. $ source activate jupyter_3.6 To help you connect to the Jupyter server, run the following command to get the hostname. Select an available port between 6818 and 11845 (in this example, 6820 is the first such available port): $ for myport in { 6818 ..11845 } ; do ! nc -z localhost ${ myport } && break ; done $ echo \"ssh -NL $myport : $( hostname ) : $myport $USER @kraken.dfci.harvard.edu\" # ssh -NL 6819:node21:6819 mtang@kraken.dfci.harvard.edu Starting teh ntoebook server: jupyter-notebook --no-browser --port = $myport --ip = '0.0.0.0'","title":"start a job on the remote server"},{"location":"kraken/kraken_jupyter/#on-your-local-computer","text":"ssh -NL 6819 :node21:6819 mtang@kraken.dfci.harvard.edu Then in your workstation/laptop browser. Make sure to copy the token from the Jupyter notebook server and update the token below. http://localhost:6819/?token=<TokenFromYourServer>","title":"on your local computer"},{"location":"kraken/kraken_jupyter/#run-juypter-notebook-on-the-remote-cluster-in-a-container","text":"The note is from https://github.com/nteract/hydrogen/issues/1184 I am running in a Singularity container on a remote cluster, working on node called 'node01'.","title":"Run Juypter Notebook on the remote cluster in a container"},{"location":"kraken/kraken_jupyter/#on-the-server","text":"edit ~/.jupyter/jupyter_notebook_config.py, commenting in c.NotebookApp.token = 'my_new_token_that_I_wrote' run jupyter notebook, here for my install: singularity exec -B /path/to/my/jupyter_notebook:/home/bmoran/jupyter_notebook /path/to/singularity/containter.for.analysis.sif jupyter-notebook --no-browser --port = 8891 --ip = 127 .0.0.1","title":"on the Server"},{"location":"kraken/kraken_jupyter/#on-your-local-machine","text":"ssh tunnel to server ssh -N -f -L 127.0.0.1:8891:127.0.0.1:8891 node01 -v -v you should be able to copy/paste the URL printed to screen on server and connect locally in browser now.","title":"on your Local machine"},{"location":"kraken/kraken_jupyter/#atom","text":"Packages -> Settings -> Manage Packages -> Hydrogen Settings [[scroll to Kernel Gateways, adding following]] [{ \"name\": \"node01\", \"options\": { \"baseUrl\": \" http://127.0.0.1:8891 \", \"token\": \"my_new_token_that_I_wrote\" } }] and restart Atom Open a file in Atom that you want to run in the kernel that you are connected to in browser Packages -> Hydrogen -> Connect to Remote Kernel [[select a gateway, click node01, enter token]] The filename open in the browser should be available to select, and run in kernel, edit and save in your current script. NB that any script in Atom can now be run in kernel...very nice!","title":"Atom"},{"location":"kraken/kraken_rstudio/","text":"How to Run Rstudio server on kraken \u00b6 install singularity and Rocker \u00b6 Also read Ming Tang's blog post https://divingintogeneticsandgenomics.rbind.io/post/run-rstudio-server-with-singularity-on-hpc/ You can not run any jobs on the login node, even conda install is not allowed. First, submit an interactive job to get a node: ssh kraken srun -t 1600 --mem = 60G -c 4 --pty bash From man srun : --time-min=<time> Set a minimum time limit on the job allocation. If specified, the job may have it's --time limit lowered to a value no lower than --time-min if doing so permits the job to begin execution ear\u2010 lier than otherwise possible. The job's time limit will not be changed after the job is allo\u2010 cated resources. This is performed by a backfill scheduling algorithm to allocate resources oth\u2010 erwise reserved for higher priority jobs. Acceptable time formats include \"minutes\", \"min\u2010 utes:seconds\", \"hours:minutes:seconds\", \"days-hours\", \"days-hours:minutes\" and \"days-hours:min\u2010 utes:seconds\". This option applies to job allocations. -c, --cpus-per-task=<ncpus> Request that ncpus be allocated per process. This may be useful if the job is multithreaded and requires more than one CPU per task for optimal performance. The default is one CPU per process. If -c is specified without -n, as many tasks will be allocated per node as possible while satis\u2010 fying the -c restriction. For instance on a cluster with 8 CPUs per node, a job request for 4 nodes and 3 CPUs per task may be allocated 3 or 6 CPUs per node (1 or 2 tasks per node) depending upon resource consumption by other jobs. Such a job may be unable to execute more than a total of 4 tasks. This option may also be useful to spawn tasks without allocating resources to the job step from the job's allocation when running multiple job steps with the --exclusive option. -N, --nodes=<minnodes[-maxnodes]> Request that a minimum of minnodes nodes be allocated to this job. A maximum node count may also be specified with maxnodes. If only one number is specified, this is used as both the minimum and maximum node count. The partition's node limits supersede those of the job. If a job's node limits are outside of the range permitted for its associated partition, the job will be left in a PENDING state. This permits possible execution at a later time, when the partition limit is changed. If a job node limit exceeds the number of nodes configured in the partition, the job will be rejected. Note that the environment variable SLURM_JOB_NUM_NODES (and SLURM_NNODES for backwards compatibility) will be set to the count of nodes actually allocated to the job. See the ENVIRONMENT VARIABLES section for more information. If -N is not specified, the default behavior is to allocate enough nodes to satisfy the requirements of the -n and -c options. The job will be allocated as many nodes as possible within the range specified and without delaying the initi\u2010 ation of the job. If number of tasks is given and a number of requested nodes is also given the number of nodes used from that request will be reduced to match that of the number of tasks if the number of nodes in the request is greater than the number of tasks. The node count specifi\u2010 cation may include a numeric value followed by a suffix of \"k\" (multiplies numeric value by 1,024) or \"m\" (multiplies numeric value by 1,048,576). This option applies to job and step allo\u2010 cations. -n, --ntasks=<number> Specify the number of tasks to run. Request that srun allocate resources for ntasks tasks. The default is one task per node, but note that the --cpus-per-task option will change this default. This option applies to job and step allocations.} After you get a computing node, you will see the prompt becomes something like: (base) [mtang@node03 mtang]$ . Note, conda singularity has problems to run. Ask the admin to install singularity on the server instead. Luckily, it is on kraken already. module load singularity pull the rocker image. read https://www.rocker-project.org/use/singularity/ cd /liulab/mtang mkdir singularity_images ; cd !$ singularity pull --name rstudio.simg docker://rocker/tidyverse To enable password authentication, set the PASSWORD environment variable and add the --auth-none=0 --auth-pam-helper-path=pam-helper options: PASSWORD = 'xyz' singularity exec --bind = /liulab/mtang rstudio.simg rserver --auth-none = 0 --auth-pam-helper-path = pam-helper --www-address = 127 .0.0.1 This will run rserver in a Singularity container. The --www-address=127.0.0.1 option binds to localhost (the default is 0.0.0.0 , or all IP addresses on the host). listening on 127.0.0.1:8787 . From your local workstation (e.g., mac book), ssh tunneling to the compute node. on your local mac: # check which ports are open sudo lsof -i -P -n | grep TCP ssh -f -L 59083 :localhost:59083 usrname@kraken.dfci.harvard.edu ssh -L 59083 :localhost:8787 -N node03 Go to your mac local browser, open localhost:59083 , type your HPC username and xyz as the password in this dummy example. You should have a Rstudio server running! set up local library \u00b6 You will want to install libraries so that the singularity container can use. If you go to your browser and inside Rstudio: > .libPaths () [1] \"/usr/local/lib/R/site-library\" \"/usr/local/lib/R/library\" You will see the library is pointing to the ones inside the container. Set up a .Renviron file in your HPC login node: if [ ! -e ${ HOME } /.Renviron ] then printf '\\nNOTE: creating ~/.Renviron file\\n\\n' echo 'R_LIBS_USER=~/R/%p-library/%v' >> ${ HOME } /.Renviron fi Now, go to Rstudio in the browser ---> Session ---> Restart R . > .libPaths () [1] \"/homes6/mtang/R/x86_64-pc-linux-gnu-library/3.6\" \"/usr/local/lib/R/site-library\" [3] \" /usr/local/lib/R/library You now see that \"/homes6/mtang/R/x86_64-pc-linux-gnu-library/3.6\" is my local library. If I install packages inside Rstudio, it will be installed there. Note : The .rstudio in your home directory /homes6/mtang/.rstudio/sessions/active/session-2cf8e174/suspended-session-data can grow large. Remember to clean it up (delete) it regularly.","title":"Run Rstudio on Kraken"},{"location":"kraken/kraken_rstudio/#how-to-run-rstudio-server-on-kraken","text":"","title":"How to Run Rstudio server on kraken"},{"location":"kraken/kraken_rstudio/#install-singularity-and-rocker","text":"Also read Ming Tang's blog post https://divingintogeneticsandgenomics.rbind.io/post/run-rstudio-server-with-singularity-on-hpc/ You can not run any jobs on the login node, even conda install is not allowed. First, submit an interactive job to get a node: ssh kraken srun -t 1600 --mem = 60G -c 4 --pty bash From man srun : --time-min=<time> Set a minimum time limit on the job allocation. If specified, the job may have it's --time limit lowered to a value no lower than --time-min if doing so permits the job to begin execution ear\u2010 lier than otherwise possible. The job's time limit will not be changed after the job is allo\u2010 cated resources. This is performed by a backfill scheduling algorithm to allocate resources oth\u2010 erwise reserved for higher priority jobs. Acceptable time formats include \"minutes\", \"min\u2010 utes:seconds\", \"hours:minutes:seconds\", \"days-hours\", \"days-hours:minutes\" and \"days-hours:min\u2010 utes:seconds\". This option applies to job allocations. -c, --cpus-per-task=<ncpus> Request that ncpus be allocated per process. This may be useful if the job is multithreaded and requires more than one CPU per task for optimal performance. The default is one CPU per process. If -c is specified without -n, as many tasks will be allocated per node as possible while satis\u2010 fying the -c restriction. For instance on a cluster with 8 CPUs per node, a job request for 4 nodes and 3 CPUs per task may be allocated 3 or 6 CPUs per node (1 or 2 tasks per node) depending upon resource consumption by other jobs. Such a job may be unable to execute more than a total of 4 tasks. This option may also be useful to spawn tasks without allocating resources to the job step from the job's allocation when running multiple job steps with the --exclusive option. -N, --nodes=<minnodes[-maxnodes]> Request that a minimum of minnodes nodes be allocated to this job. A maximum node count may also be specified with maxnodes. If only one number is specified, this is used as both the minimum and maximum node count. The partition's node limits supersede those of the job. If a job's node limits are outside of the range permitted for its associated partition, the job will be left in a PENDING state. This permits possible execution at a later time, when the partition limit is changed. If a job node limit exceeds the number of nodes configured in the partition, the job will be rejected. Note that the environment variable SLURM_JOB_NUM_NODES (and SLURM_NNODES for backwards compatibility) will be set to the count of nodes actually allocated to the job. See the ENVIRONMENT VARIABLES section for more information. If -N is not specified, the default behavior is to allocate enough nodes to satisfy the requirements of the -n and -c options. The job will be allocated as many nodes as possible within the range specified and without delaying the initi\u2010 ation of the job. If number of tasks is given and a number of requested nodes is also given the number of nodes used from that request will be reduced to match that of the number of tasks if the number of nodes in the request is greater than the number of tasks. The node count specifi\u2010 cation may include a numeric value followed by a suffix of \"k\" (multiplies numeric value by 1,024) or \"m\" (multiplies numeric value by 1,048,576). This option applies to job and step allo\u2010 cations. -n, --ntasks=<number> Specify the number of tasks to run. Request that srun allocate resources for ntasks tasks. The default is one task per node, but note that the --cpus-per-task option will change this default. This option applies to job and step allocations.} After you get a computing node, you will see the prompt becomes something like: (base) [mtang@node03 mtang]$ . Note, conda singularity has problems to run. Ask the admin to install singularity on the server instead. Luckily, it is on kraken already. module load singularity pull the rocker image. read https://www.rocker-project.org/use/singularity/ cd /liulab/mtang mkdir singularity_images ; cd !$ singularity pull --name rstudio.simg docker://rocker/tidyverse To enable password authentication, set the PASSWORD environment variable and add the --auth-none=0 --auth-pam-helper-path=pam-helper options: PASSWORD = 'xyz' singularity exec --bind = /liulab/mtang rstudio.simg rserver --auth-none = 0 --auth-pam-helper-path = pam-helper --www-address = 127 .0.0.1 This will run rserver in a Singularity container. The --www-address=127.0.0.1 option binds to localhost (the default is 0.0.0.0 , or all IP addresses on the host). listening on 127.0.0.1:8787 . From your local workstation (e.g., mac book), ssh tunneling to the compute node. on your local mac: # check which ports are open sudo lsof -i -P -n | grep TCP ssh -f -L 59083 :localhost:59083 usrname@kraken.dfci.harvard.edu ssh -L 59083 :localhost:8787 -N node03 Go to your mac local browser, open localhost:59083 , type your HPC username and xyz as the password in this dummy example. You should have a Rstudio server running!","title":"install singularity and Rocker"},{"location":"kraken/kraken_rstudio/#set-up-local-library","text":"You will want to install libraries so that the singularity container can use. If you go to your browser and inside Rstudio: > .libPaths () [1] \"/usr/local/lib/R/site-library\" \"/usr/local/lib/R/library\" You will see the library is pointing to the ones inside the container. Set up a .Renviron file in your HPC login node: if [ ! -e ${ HOME } /.Renviron ] then printf '\\nNOTE: creating ~/.Renviron file\\n\\n' echo 'R_LIBS_USER=~/R/%p-library/%v' >> ${ HOME } /.Renviron fi Now, go to Rstudio in the browser ---> Session ---> Restart R . > .libPaths () [1] \"/homes6/mtang/R/x86_64-pc-linux-gnu-library/3.6\" \"/usr/local/lib/R/site-library\" [3] \" /usr/local/lib/R/library You now see that \"/homes6/mtang/R/x86_64-pc-linux-gnu-library/3.6\" is my local library. If I install packages inside Rstudio, it will be installed there. Note : The .rstudio in your home directory /homes6/mtang/.rstudio/sessions/active/session-2cf8e174/suspended-session-data can grow large. Remember to clean it up (delete) it regularly.","title":"set up local library"},{"location":"kraken/kraken_sshfs/","text":"sshfs to mount remote kraken cluster folder to local computer \u00b6 follow https://www.digitalocean.com/community/tutorials/how-to-use-sshfs-to-mount-remote-file-systems-over-ssh on mac \u00b6 brew install --cask osxfuse brew install sshfs ## create a config file $ cat ~/.ssh/config Host * ServerAliveInterval 60 Host kraken HostName kraken.dfci.harvard.edu User mtang RemoteForward 52698 localhost:52698 cd ~ mkdir kraken ## mount the remote dir to local ~/kraken sshfs -o allow_other,default_permissions kraken:/liulab/mtang kraken ## umount umount kraken","title":"mount kraken folder to local computer"},{"location":"kraken/kraken_sshfs/#sshfs-to-mount-remote-kraken-cluster-folder-to-local-computer","text":"follow https://www.digitalocean.com/community/tutorials/how-to-use-sshfs-to-mount-remote-file-systems-over-ssh","title":"sshfs to mount remote kraken cluster folder to local computer"},{"location":"kraken/kraken_sshfs/#on-mac","text":"brew install --cask osxfuse brew install sshfs ## create a config file $ cat ~/.ssh/config Host * ServerAliveInterval 60 Host kraken HostName kraken.dfci.harvard.edu User mtang RemoteForward 52698 localhost:52698 cd ~ mkdir kraken ## mount the remote dir to local ~/kraken sshfs -o allow_other,default_permissions kraken:/liulab/mtang kraken ## umount umount kraken","title":"on mac"},{"location":"miscellaneous/inkscape/","text":"inkscape \u00b6 There are two types of images: bitmap image and vector image. See this to get the idea of differences between them: https://www.lifewire.com/vector-and-bitmap-images-1701238 Photoshop for bitmap images (e.g. gel pictures) Illustrator for vector images (e.g. plots from python/R in pdf format) I use free tools: * gimp for bigmap images. * inkscape for vector images. Tutorials \u00b6 tutorial for inkscape .","title":"Making figures"},{"location":"miscellaneous/inkscape/#inkscape","text":"There are two types of images: bitmap image and vector image. See this to get the idea of differences between them: https://www.lifewire.com/vector-and-bitmap-images-1701238 Photoshop for bitmap images (e.g. gel pictures) Illustrator for vector images (e.g. plots from python/R in pdf format) I use free tools: * gimp for bigmap images. * inkscape for vector images.","title":"inkscape"},{"location":"miscellaneous/inkscape/#tutorials","text":"tutorial for inkscape .","title":"Tutorials"},{"location":"setup/hpc-info/","text":"HPC information for Liu lab \u00b6 Kraken server \u00b6 Nikos George (nikos[at]ds.dfci.harvard.edu)is the head of our Department Computing. Contact Nikos to get the Kraken (dept server) account. documentation is at http://dscomputing.dfci.harvard.edu/index.php/kraken/ you can only access when you login the parterns' VPN (pvc.partners.org/saml). Nikos will ask for your ssh public key. Generate a key pair by $ ssh-keygen -b 2048 send ~/.ssh/id_rsa.pub to Nikos. Remember sending only out the public key, not the private key id_sra . After you\u2019ve added your public key to the remote host, try logging in a few times. You\u2019ll notice that you keep getting prompted for your SSH key\u2019s password. If you\u2019re scratching your head wondering how this saves time, there\u2019s one more trick to know: ssh-agent . The ssh-agent program runs in the background on your local machine,and manages your SSH key(s). ssh-agent allows you to use your keys without entering their passwords each time\u2014exactly what we want when we frequently connect to servers. SSH agent is usually already running on Unix-based systems, but if not, you can use eval ssh-agent to start it. Then, to tell ssh-agent about our key, we use ssh-add : From bioinformatics data skills . $ ssh-add You have write permission to /liulab which is a lab share and to /cluster/liulab which is on a fast (IO intensive) cluster filesystem, strictly to be used as scratch space. Please note that /cluster/liulab is only available to the work nodes, but you can point your TMP_DIR to it Home Directory Storage \u00b6 Each user is given a home directory (/homes/username) that is mounted on all cluster nodes. It has a size limit of 20Gb. Please use it for basic login scripts and simple submit jobs. It is backed up daily, and backups are kept for 3 months. Lab Storage \u00b6 Each user has write permissions to their appropriate lab share (/name-of-lab.) Lab shares are mounted on all cluster nodes and can also be mounted on desktops and laptops. Size limits depend on the particular lab, this is where you put your regular data and work files. It is backed up weekly, and backups are kept for 3 months High Performance Scratch Storage \u00b6 Every lab has access to our high performance scratch space (/cluster/name-of-lab) Each user can create their own folder. This filesystem is managed by the GPFS parallel filesystem and is apropriate for data intensive jobs. It is mounted on all work nodes, but not on the head nodes. It is considered as a temporary storage. Files are not backed up and if the storage fills up we may delete any files, so once your analysis has been completed please move your files to your lab share. Local tmp storage \u00b6 Every work node has a small storage partition (approximately 100Gb) that is suitable for temp files (/tmp). This partition is not backed up and files can be deleted at any time. It is best not to use it since it is specific to the node and not shared across nodes. If your application is contained within the same node you can point TMPDIR to it. Iris/Daisy server \u00b6 Contact Annie Ng (annie[at]ds.dfci.harvard.edu) to connect to the Iris / Daisy (LiuLab server, cistrome.org). # if connected to VPN for local access ssh username@155.52.47.121 -p 33001 # if not connected to VPN ssh username@cistrome.org -p 33001 the /home/username folder has a 500MB quota so do create a /project/dev/username folder and put your work files there. Hosting files in Iris \u00b6 Thanks Changxin for the tip. after log into iris : cd /project/dev/username mkdir public_html ## put any file you want to share inside the public_html folder cd public_html touch test.txt ## soft link to your home directory ln -s /project/dev/username/public_html /home/username/public_html The test.txt should be available in the web browser with address: http://cistrome.org/~username/test.txt","title":"set up HPC accounts"},{"location":"setup/hpc-info/#hpc-information-for-liu-lab","text":"","title":"HPC information for Liu lab"},{"location":"setup/hpc-info/#kraken-server","text":"Nikos George (nikos[at]ds.dfci.harvard.edu)is the head of our Department Computing. Contact Nikos to get the Kraken (dept server) account. documentation is at http://dscomputing.dfci.harvard.edu/index.php/kraken/ you can only access when you login the parterns' VPN (pvc.partners.org/saml). Nikos will ask for your ssh public key. Generate a key pair by $ ssh-keygen -b 2048 send ~/.ssh/id_rsa.pub to Nikos. Remember sending only out the public key, not the private key id_sra . After you\u2019ve added your public key to the remote host, try logging in a few times. You\u2019ll notice that you keep getting prompted for your SSH key\u2019s password. If you\u2019re scratching your head wondering how this saves time, there\u2019s one more trick to know: ssh-agent . The ssh-agent program runs in the background on your local machine,and manages your SSH key(s). ssh-agent allows you to use your keys without entering their passwords each time\u2014exactly what we want when we frequently connect to servers. SSH agent is usually already running on Unix-based systems, but if not, you can use eval ssh-agent to start it. Then, to tell ssh-agent about our key, we use ssh-add : From bioinformatics data skills . $ ssh-add You have write permission to /liulab which is a lab share and to /cluster/liulab which is on a fast (IO intensive) cluster filesystem, strictly to be used as scratch space. Please note that /cluster/liulab is only available to the work nodes, but you can point your TMP_DIR to it","title":"Kraken server"},{"location":"setup/hpc-info/#home-directory-storage","text":"Each user is given a home directory (/homes/username) that is mounted on all cluster nodes. It has a size limit of 20Gb. Please use it for basic login scripts and simple submit jobs. It is backed up daily, and backups are kept for 3 months.","title":"Home Directory Storage"},{"location":"setup/hpc-info/#lab-storage","text":"Each user has write permissions to their appropriate lab share (/name-of-lab.) Lab shares are mounted on all cluster nodes and can also be mounted on desktops and laptops. Size limits depend on the particular lab, this is where you put your regular data and work files. It is backed up weekly, and backups are kept for 3 months","title":"Lab Storage"},{"location":"setup/hpc-info/#high-performance-scratch-storage","text":"Every lab has access to our high performance scratch space (/cluster/name-of-lab) Each user can create their own folder. This filesystem is managed by the GPFS parallel filesystem and is apropriate for data intensive jobs. It is mounted on all work nodes, but not on the head nodes. It is considered as a temporary storage. Files are not backed up and if the storage fills up we may delete any files, so once your analysis has been completed please move your files to your lab share.","title":"High Performance Scratch Storage"},{"location":"setup/hpc-info/#local-tmp-storage","text":"Every work node has a small storage partition (approximately 100Gb) that is suitable for temp files (/tmp). This partition is not backed up and files can be deleted at any time. It is best not to use it since it is specific to the node and not shared across nodes. If your application is contained within the same node you can point TMPDIR to it.","title":"Local tmp storage"},{"location":"setup/hpc-info/#irisdaisy-server","text":"Contact Annie Ng (annie[at]ds.dfci.harvard.edu) to connect to the Iris / Daisy (LiuLab server, cistrome.org). # if connected to VPN for local access ssh username@155.52.47.121 -p 33001 # if not connected to VPN ssh username@cistrome.org -p 33001 the /home/username folder has a 500MB quota so do create a /project/dev/username folder and put your work files there.","title":"Iris/Daisy server"},{"location":"setup/hpc-info/#hosting-files-in-iris","text":"Thanks Changxin for the tip. after log into iris : cd /project/dev/username mkdir public_html ## put any file you want to share inside the public_html folder cd public_html touch test.txt ## soft link to your home directory ln -s /project/dev/username/public_html /home/username/public_html The test.txt should be available in the web browser with address: http://cistrome.org/~username/test.txt","title":"Hosting files in Iris"},{"location":"setup/setup_macos/","text":"set up my new mac pro \u00b6 old post https://divingintogeneticsandgenomics.rbind.io/post/set-up-my-new-mac-laptop/ download iterm \u00b6 configure color iTerm \u2192 Preferences \u2192 Profiles \u2192 colors -> Color Presets \u2192 Tango Dark By default, word jumps (option + \u2192 or \u2190) and word deletions (option + backspace) do not work. To enable these, go to \"iTerm \u2192 Preferences \u2192 Profiles \u2192 Keys \u2192 Load Preset... \u2192 Natural Text Editing \u2192 Boom! Head explodes\" iterm2 tips \u00b6 press command + click the file inside the terminal to open it! install oh-my-zsh \u00b6 sh -c \" $( curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh ) \" zsh syntax highlighting \u00b6 https://github.com/zsh-users/zsh-syntax-highlighting/blob/master/INSTALL.md Oh-my-zsh : Clone this repository in oh-my-zsh's plugins directory: git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ ZSH_CUSTOM :- ~/.oh-my-zsh/custom } /plugins/zsh-syntax-highlighting Activate the plugin in ~/.zshrc: my .zshrc has a plugin git , add it after that plugins=(git zsh-syntax-highlighting) https://github.com/zsh-users/zsh-syntax-highlighting/issues/530 shell integration \u00b6 https://iterm2.com/documentation-shell-integration.html logout and then login view images inside terminal \u00b6 put imgcat to your ~/bin . and add export PATH=$PATH:~/bin to your .zshrc https://www.iterm2.com/documentation-images.html A note on login shell and interactive shell. https://codingbee.net/rhcsa/rhcsa-starting-a-login-shell-or-interactive-shell-using-the-switch-user-su-command install conda \u00b6 conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge install sublime, rmate \u00b6 install R and Rstudio \u00b6 install brew \u00b6 /bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh ) \" brew install ncdu","title":"set up MacOS"},{"location":"setup/setup_macos/#set-up-my-new-mac-pro","text":"old post https://divingintogeneticsandgenomics.rbind.io/post/set-up-my-new-mac-laptop/","title":"set up my new mac pro"},{"location":"setup/setup_macos/#download-iterm","text":"configure color iTerm \u2192 Preferences \u2192 Profiles \u2192 colors -> Color Presets \u2192 Tango Dark By default, word jumps (option + \u2192 or \u2190) and word deletions (option + backspace) do not work. To enable these, go to \"iTerm \u2192 Preferences \u2192 Profiles \u2192 Keys \u2192 Load Preset... \u2192 Natural Text Editing \u2192 Boom! Head explodes\"","title":"download iterm"},{"location":"setup/setup_macos/#iterm2-tips","text":"press command + click the file inside the terminal to open it!","title":"iterm2 tips"},{"location":"setup/setup_macos/#install-oh-my-zsh","text":"sh -c \" $( curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh ) \"","title":"install oh-my-zsh"},{"location":"setup/setup_macos/#zsh-syntax-highlighting","text":"https://github.com/zsh-users/zsh-syntax-highlighting/blob/master/INSTALL.md Oh-my-zsh : Clone this repository in oh-my-zsh's plugins directory: git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ ZSH_CUSTOM :- ~/.oh-my-zsh/custom } /plugins/zsh-syntax-highlighting Activate the plugin in ~/.zshrc: my .zshrc has a plugin git , add it after that plugins=(git zsh-syntax-highlighting) https://github.com/zsh-users/zsh-syntax-highlighting/issues/530","title":"zsh syntax highlighting"},{"location":"setup/setup_macos/#shell-integration","text":"https://iterm2.com/documentation-shell-integration.html logout and then login","title":"shell integration"},{"location":"setup/setup_macos/#view-images-inside-terminal","text":"put imgcat to your ~/bin . and add export PATH=$PATH:~/bin to your .zshrc https://www.iterm2.com/documentation-images.html A note on login shell and interactive shell. https://codingbee.net/rhcsa/rhcsa-starting-a-login-shell-or-interactive-shell-using-the-switch-user-su-command","title":"view images inside terminal"},{"location":"setup/setup_macos/#install-conda","text":"conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge","title":"install conda"},{"location":"setup/setup_macos/#install-sublime-rmate","text":"","title":"install sublime, rmate"},{"location":"setup/setup_macos/#install-r-and-rstudio","text":"","title":"install R and Rstudio"},{"location":"setup/setup_macos/#install-brew","text":"/bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh ) \" brew install ncdu","title":"install brew"}]}